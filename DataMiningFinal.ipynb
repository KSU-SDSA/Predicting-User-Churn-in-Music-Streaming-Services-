{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AuI-uW9nm7pY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KThtt2jTB0Y4",
        "outputId": "3d5393ae-9a54-49f3-c76a-17a576646be1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import warnings\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "Yf3GVIGNCC-Y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "\n",
        "def load_data(file_path):\n",
        "    CHUNKSIZE = 100000\n",
        "    # Removing 'ts': 'int64' to allow pandas to infer the type for ts\n",
        "    dtypes = {'userId': 'category', 'page': 'category'}\n",
        "    return pd.concat(pd.read_json(file_path, lines=True, chunksize=CHUNKSIZE, dtype=dtypes))"
      ],
      "metadata": {
        "id": "jm7_l_dECIER"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploratory Data Analysis\n",
        "\n",
        "def perform_eda(df):\n",
        "    print(\"\\n===== EXPLORATORY DATA ANALYSIS =====\")\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(\"\\nData types:\")\n",
        "    print(df.dtypes)\n",
        "\n",
        "    print(\"\\nMissing values per column:\")\n",
        "    print(df.isnull().sum())\n",
        "\n",
        "    print(\"\\nBasic statistics:\")\n",
        "    print(df.describe())\n",
        "\n",
        "    print(\"\\nUnique values in categorical columns:\")\n",
        "    for col in df.select_dtypes(include=['category', 'object']).columns:\n",
        "        print(f\"{col}: {df[col].nunique()} unique values\")\n",
        "\n",
        "    print(\"\\nPage distribution:\")\n",
        "    page_counts = df['page'].value_counts()\n",
        "    print(page_counts)\n",
        "\n",
        "    # Create EDA visualizations\n",
        "    os.makedirs(\"plots/eda\", exist_ok=True)\n",
        "\n",
        "    # Plot page distribution\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    page_counts.head(15).plot(kind='bar')\n",
        "    plt.title('Top 15 Pages by Frequency')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xlabel('Page')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"plots/eda/page_distribution.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Plot user levels\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    df['level'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
        "    plt.title('User Subscription Level Distribution')\n",
        "    plt.ylabel('')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"plots/eda/user_levels.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Plot gender distribution\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    df['gender'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
        "    plt.title('Gender Distribution')\n",
        "    plt.ylabel('')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"plots/eda/gender_distribution.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Convert 'ts' to datetime before time analysis\n",
        "    df['ts'] = pd.to_datetime(df['ts'], unit='ms')\n",
        "\n",
        "    # Time analysis\n",
        "    df['hour'] = df['ts'].dt.hour\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    df['hour'].value_counts().sort_index().plot(kind='bar')\n",
        "    plt.title('User Activity by Hour of Day')\n",
        "    plt.xlabel('Hour of Day')\n",
        "    plt.ylabel('Number of Actions')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"plots/eda/activity_by_hour.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # User agent analysis\n",
        "    if 'userAgent' in df.columns:\n",
        "        # Extract device type from user agent\n",
        "        def get_device_type(user_agent):\n",
        "            if pd.isna(user_agent):\n",
        "                return \"Unknown\"\n",
        "            ua = user_agent.lower()\n",
        "            if 'iphone' in ua or 'ipad' in ua or 'ios' in ua:\n",
        "                return \"iOS\"\n",
        "            elif 'android' in ua:\n",
        "                return \"Android\"\n",
        "            elif 'windows' in ua or 'mac' in ua or 'linux' in ua:\n",
        "                return \"Desktop\"\n",
        "            else:\n",
        "                return \"Other\"\n",
        "\n",
        "        df['device_type'] = df['userAgent'].apply(get_device_type)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        df['device_type'].value_counts().plot(kind='bar')\n",
        "        plt.title('Device Type Distribution')\n",
        "        plt.xlabel('Device Type')\n",
        "        plt.ylabel('Count')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"plots/eda/device_distribution.png\")\n",
        "        plt.close()\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "zB2aRtmyCIHO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess and Feature Engineering\n",
        "\n",
        "def preprocess_data(df):\n",
        "    print(\"\\nPreprocessing & Feature Engineering...\")\n",
        "    df = df[df['userId'] != '']\n",
        "    df['churn'] = (df['page'] == 'Cancellation Confirmation').astype(int)\n",
        "    df['ts'] = pd.to_datetime(df['ts'], unit='ms')\n",
        "    df['registration'] = pd.to_datetime(df['registration'], unit='ms')\n",
        "    df['session_duration'] = df.groupby(['userId', 'sessionId'])['ts'].transform(lambda x: (x.max() - x.min()).total_seconds())\n",
        "\n",
        "    # Enhanced feature engineering\n",
        "    if 'page' in df.columns:\n",
        "        df['is_home'] = (df['page'] == 'Home').astype(int)\n",
        "        df['is_settings'] = (df['page'] == 'Settings').astype(int)\n",
        "        df['is_help'] = (df['page'] == 'Help').astype(int)\n",
        "        df['is_downgrade'] = (df['page'] == 'Downgrade').astype(int)\n",
        "        df['is_upgrade'] = (df['page'] == 'Upgrade').astype(int)\n",
        "        df['is_about'] = (df['page'] == 'About').astype(int)\n",
        "        df['is_next_song'] = (df['page'] == 'NextSong').astype(int)\n",
        "        df['is_thumbs_up'] = (df['page'] == 'Thumbs Up').astype(int)\n",
        "        df['is_thumbs_down'] = (df['page'] == 'Thumbs Down').astype(int)\n",
        "        df['is_error'] = (df['page'] == 'Error').astype(int)\n",
        "\n",
        "    current_time = df['ts'].max()\n",
        "\n",
        "    agg_funcs = {\n",
        "        'length': ['sum', 'mean', 'count', 'max', 'std'],\n",
        "        'sessionId': 'nunique',\n",
        "        'ts': [\n",
        "            lambda x: (x.max() - x.min()).total_seconds(),\n",
        "            lambda x: (current_time - x.max()).total_seconds()\n",
        "        ],\n",
        "        'session_duration': ['mean', 'max', 'std'],\n",
        "        'churn': 'max',\n",
        "        'itemInSession': ['max', 'mean', 'std'],\n",
        "        'page': 'nunique',\n",
        "        'artist': 'nunique',\n",
        "        'song': 'nunique',\n",
        "        'userAgent': 'nunique',\n",
        "        'method': 'nunique'\n",
        "    }\n",
        "\n",
        "    for col in ['is_home', 'is_settings', 'is_help', 'is_downgrade', 'is_upgrade',\n",
        "                'is_about', 'is_next_song', 'is_thumbs_up', 'is_thumbs_down', 'is_error']:\n",
        "        if col in df.columns:\n",
        "            agg_funcs[col] = 'sum'\n",
        "\n",
        "    user_features = df.groupby('userId').agg(agg_funcs)\n",
        "    user_features.columns = ['_'.join([col[0], str(col[1])]) if isinstance(col, tuple) else col\n",
        "                              for col in user_features.columns]\n",
        "\n",
        "    column_mapping = {\n",
        "        'length_sum': 'total_listen',\n",
        "        'length_mean': 'avg_listen',\n",
        "        'length_count': 'listen_count',\n",
        "        'length_max': 'max_listen',\n",
        "        'length_std': 'std_listen',\n",
        "        'sessionId_nunique': 'sessions',\n",
        "        'ts_<lambda_0>': 'active_duration',\n",
        "        'ts_<lambda_1>': 'recency',\n",
        "        'session_duration_mean': 'avg_session_duration',\n",
        "        'session_duration_max': 'max_session_duration',\n",
        "        'session_duration_std': 'std_session_duration',\n",
        "        'churn_max': 'churn',\n",
        "        'itemInSession_max': 'max_item_in_session',\n",
        "        'itemInSession_mean': 'avg_item_in_session',\n",
        "        'itemInSession_std': 'std_item_in_session',\n",
        "        'page_nunique': 'unique_pages',\n",
        "        'artist_nunique': 'unique_artists',\n",
        "        'song_nunique': 'unique_songs',\n",
        "        'userAgent_nunique': 'unique_agents',\n",
        "        'method_nunique': 'unique_methods'\n",
        "    }\n",
        "\n",
        "    user_features = user_features.rename(columns=column_mapping)\n",
        "    user_features = user_features.reset_index()\n",
        "\n",
        "    last_levels = df.sort_values('ts').groupby('userId')['level'].last()\n",
        "    genders = df.groupby('userId')['gender'].first()\n",
        "    user_features['is_paid'] = (last_levels == 'paid').astype(int)\n",
        "    user_features['is_male'] = (genders == 'M').astype(int)\n",
        "\n",
        "    reg_dates = df.groupby('userId')['registration'].first()\n",
        "    last_ts = df.groupby('userId')['ts'].max()\n",
        "    user_features['membership_days'] = (last_ts - reg_dates).dt.total_seconds() / (3600 * 24)\n",
        "\n",
        "    if 'is_next_song_sum' in user_features.columns:\n",
        "        user_features['next_song_ratio'] = user_features['is_next_song_sum'] / user_features['listen_count']\n",
        "\n",
        "    if 'is_thumbs_up_sum' in user_features.columns and 'is_thumbs_down_sum' in user_features.columns:\n",
        "        user_features['satisfaction_ratio'] = user_features['is_thumbs_up_sum'] / (user_features['is_thumbs_down_sum'] + 1)\n",
        "\n",
        "    if 'is_error_sum' in user_features.columns:\n",
        "        user_features['error_rate'] = user_features['is_error_sum'] / user_features['listen_count']\n",
        "\n",
        "    user_features['artist_diversity'] = user_features['unique_artists'] / user_features['listen_count']\n",
        "    user_features['song_diversity'] = user_features['unique_songs'] / user_features['listen_count']\n",
        "    user_features['usage_frequency'] = user_features['sessions'] / (user_features['membership_days'] + 1)\n",
        "    user_features['avg_session_items'] = user_features['listen_count'] / (user_features['sessions'] + 1)\n",
        "\n",
        "    user_features = user_features.fillna(0).replace([np.inf, -np.inf], 0)\n",
        "\n",
        "    print(f\"Created {len(user_features.columns)} features for {len(user_features)} users\")\n",
        "    return user_features\n"
      ],
      "metadata": {
        "id": "jU63bH7-CIK3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clustering\n",
        "\n",
        "def cluster_users(user_features):\n",
        "    print(\"\\nClustering users into behavioral segments...\")\n",
        "    cluster_cols = [\n",
        "        'total_listen', 'avg_listen', 'sessions', 'active_duration',\n",
        "        'membership_days', 'unique_artists', 'unique_songs',\n",
        "        'avg_session_duration', 'is_paid'\n",
        "    ]\n",
        "    cluster_cols = [col for col in cluster_cols if col in user_features.columns]\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(user_features[cluster_cols])\n",
        "    n_components_range = range(2, 11)\n",
        "    bic_scores = []\n",
        "\n",
        "    for n_components in n_components_range:\n",
        "        gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
        "        gmm.fit(X_scaled)\n",
        "        bic_scores.append(gmm.bic(X_scaled))\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(n_components_range, bic_scores, marker='o')\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('BIC Score')\n",
        "    plt.title('BIC Score by Number of Components')\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    os.makedirs(\"plots\", exist_ok=True)\n",
        "    plt.savefig(\"plots/bic_score.png\")\n",
        "    plt.close()\n",
        "\n",
        "    optimal_n_components = n_components_range[np.argmin(bic_scores)]\n",
        "    print(f\"Optimal number of clusters: {optimal_n_components}\")\n",
        "    gmm = GaussianMixture(n_components=optimal_n_components, random_state=42)\n",
        "    user_features['cluster'] = gmm.fit_predict(X_scaled)\n",
        "\n",
        "    cluster_analysis = user_features.groupby('cluster').agg({\n",
        "        'churn': 'mean',\n",
        "        'total_listen': 'mean',\n",
        "        'avg_listen': 'mean',\n",
        "        'sessions': 'mean',\n",
        "        'active_duration': 'mean',\n",
        "        'membership_days': 'mean',\n",
        "        'is_paid': 'mean',\n",
        "        'unique_artists': 'mean',\n",
        "        'unique_songs': 'mean',\n",
        "        'avg_session_duration': 'mean',\n",
        "        'userId': 'count'\n",
        "    }).rename(columns={'userId': 'count'})\n",
        "\n",
        "    cluster_analysis = cluster_analysis.sort_values('churn', ascending=False)\n",
        "    print(\"\\n===== CLUSTER ANALYSIS =====\")\n",
        "    print(cluster_analysis)\n",
        "    cluster_analysis.to_csv(\"cluster_analysis.csv\")\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.heatmap(cluster_analysis.drop('count', axis=1), cmap='coolwarm', annot=True, fmt=\".2f\")\n",
        "    plt.title('Cluster Characteristics')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"plots/cluster_characteristics.png\")\n",
        "    plt.close()\n",
        "\n",
        "    interpret_clusters(cluster_analysis)\n",
        "    return user_features"
      ],
      "metadata": {
        "id": "s243N-RdCIMJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cluster Interpretation\n",
        "\n",
        "def interpret_clusters(cluster_analysis):\n",
        "    print(\"\\n===== CLUSTER INTERPRETATION =====\")\n",
        "    cluster_interpretations = {}\n",
        "    risk_levels = {}\n",
        "\n",
        "    for cluster in cluster_analysis.index:\n",
        "        data = cluster_analysis.loc[cluster]\n",
        "        churn_rate = data['churn']\n",
        "        if churn_rate >= 0.4:\n",
        "            risk_level = \"High Risk\"\n",
        "        elif churn_rate >= 0.2:\n",
        "            risk_level = \"Medium Risk\"\n",
        "        else:\n",
        "            risk_level = \"Low Risk\"\n",
        "        risk_levels[cluster] = risk_level\n",
        "\n",
        "        is_paid = \"Paid\" if data['is_paid'] > 0.5 else \"Free\"\n",
        "\n",
        "        if data['active_duration'] > cluster_analysis['active_duration'].median():\n",
        "            activity = \"High activity\"\n",
        "        else:\n",
        "            activity = \"Low activity\"\n",
        "\n",
        "        if data['membership_days'] > cluster_analysis['membership_days'].median():\n",
        "            tenure = \"Long-term\"\n",
        "        else:\n",
        "            tenure = \"New\"\n",
        "\n",
        "        if data['unique_artists'] > cluster_analysis['unique_artists'].median():\n",
        "            diversity = \"Diverse taste\"\n",
        "        else:\n",
        "            diversity = \"Focused taste\"\n",
        "\n",
        "        description = f\"{risk_level} - {is_paid} users with {activity}, {tenure}, {diversity}\"\n",
        "        recommendations = []\n",
        "        if risk_level == \"High Risk\":\n",
        "            if is_paid == \"Paid\":\n",
        "                recommendations.append(\"Offer discounts or premium features\")\n",
        "            else:\n",
        "                recommendations.append(\"Engage with targeted content recommendations\")\n",
        "            if activity == \"Low activity\":\n",
        "                recommendations.append(\"Send re-engagement emails with personalized playlists\")\n",
        "            if tenure == \"New\":\n",
        "                recommendations.append(\"Improve onboarding experience\")\n",
        "            else:\n",
        "                recommendations.append(\"Highlight long-term benefits and loyalty rewards\")\n",
        "        elif risk_level == \"Medium Risk\":\n",
        "            if diversity == \"Focused taste\":\n",
        "                recommendations.append(\"Recommend new artists similar to their favorites\")\n",
        "            else:\n",
        "                recommendations.append(\"Create personalized discovery playlists\")\n",
        "            if is_paid == \"Free\":\n",
        "                recommendations.append(\"Highlight premium benefits\")\n",
        "        else:\n",
        "            recommendations.append(\"Maintain engagement with new features\")\n",
        "            recommendations.append(\"Leverage these users for referrals\")\n",
        "\n",
        "        cluster_interpretations[cluster] = {\n",
        "            'description': description,\n",
        "            'size': data['count'],\n",
        "            'churn_rate': f\"{churn_rate:.2%}\",\n",
        "            'recommendations': recommendations\n",
        "        }\n",
        "\n",
        "    for cluster, interp in cluster_interpretations.items():\n",
        "        print(f\"\\nCluster {cluster}: {interp['description']}\")\n",
        "        print(f\"Size: {interp['size']} users\")\n",
        "        print(f\"Churn Rate: {interp['churn_rate']}\")\n",
        "        print(\"Recommendations:\")\n",
        "        for i, rec in enumerate(interp['recommendations'], 1):\n",
        "            print(f\"  {i}. {rec}\")\n",
        "\n",
        "    with open(\"cluster_interpretations.txt\", \"w\") as f:\n",
        "        f.write(\"===== CLUSTER INTERPRETATIONS =====\\n\\n\")\n",
        "        for cluster, interp in cluster_interpretations.items():\n",
        "            f.write(f\"Cluster {cluster}: {interp['description']}\\n\")\n",
        "            f.write(f\"Size: {interp['size']} users\\n\")\n",
        "            f.write(f\"Churn Rate: {interp['churn_rate']}\\n\")\n",
        "            f.write(\"Recommendations:\\n\")\n",
        "            for i, rec in enumerate(interp['recommendations'], 1):\n",
        "                f.write(f\"  {i}. {rec}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "    return cluster_interpretations, risk_levels"
      ],
      "metadata": {
        "id": "rDmZpLe9CtsB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization\n",
        "\n",
        "def visualize_data(df):\n",
        "    print(\"\\nGenerating visualizations...\")\n",
        "    os.makedirs(\"plots\", exist_ok=True)\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.countplot(data=df, x='churn')\n",
        "    plt.title(\"Churn Distribution\")\n",
        "    plt.xticks([0, 1], ['Not Churned', 'Churned'])\n",
        "    plt.ylabel(\"Number of Users\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"plots/churn_distribution.png\")\n",
        "    plt.close()\n",
        "\n",
        "    for col in ['total_listen', 'avg_listen', 'sessions', 'active_duration', 'membership_days']:\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        sns.boxplot(data=df, x='churn', y=col)\n",
        "        plt.title(f\"{col} vs Churn\")\n",
        "        plt.xticks([0, 1], ['Not Churned', 'Churned'])\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"plots/{col}_vs_churn.png\")\n",
        "        plt.close()\n",
        "\n",
        "    corr = df.drop(columns=['userId']).corr()\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(corr.iloc[:20, :20], annot=True, fmt=\".2f\", cmap='coolwarm', center=0)\n",
        "    plt.title(\"Top Feature Correlation Heatmap\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"plots/correlation_heatmap.png\")\n",
        "    plt.close()\n",
        "\n",
        "    if 'cluster' in df.columns:\n",
        "        cluster_churn = df.groupby('cluster')['churn'].mean().reset_index()\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        sns.barplot(data=cluster_churn, x='cluster', y='churn')\n",
        "        plt.title(\"Churn Rate by Cluster\")\n",
        "        plt.ylabel(\"Churn Rate\")\n",
        "        plt.xlabel(\"Cluster\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"plots/cluster_vs_churn.png\")\n",
        "        plt.close()\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        cluster_counts = df['cluster'].value_counts().sort_index()\n",
        "        ax = cluster_counts.plot(kind='bar')\n",
        "        plt.title(\"Number of Users by Cluster\")\n",
        "        plt.xlabel(\"Cluster\")\n",
        "        plt.ylabel(\"Number of Users\")\n",
        "        total = cluster_counts.sum()\n",
        "        for i, count in enumerate(cluster_counts):\n",
        "            percentage = 100 * count / total\n",
        "            ax.annotate(f\"{percentage:.1f}%\", (i, count), ha='center', va='bottom')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"plots/cluster_distribution.png\")\n",
        "        plt.close()\n",
        "\n",
        "        for feature in ['total_listen', 'avg_listen', 'membership_days', 'is_paid']:\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            sns.boxplot(data=df, x='cluster', y=feature)\n",
        "            plt.title(f\"{feature} Distribution by Cluster\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"plots/cluster_{feature}.png\")\n",
        "            plt.close()"
      ],
      "metadata": {
        "id": "dCW3htTTCtuw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "def tune_hyperparameters(X_train, y_train):\n",
        "    print(\"\\n===== HYPERPARAMETER TUNING =====\")\n",
        "\n",
        "    # Logistic Regression\n",
        "    print(\"\\nTuning Logistic Regression...\")\n",
        "    lr_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('model', LogisticRegression(max_iter=1000))\n",
        "    ])\n",
        "\n",
        "    lr_param_grid = {\n",
        "        'model__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "        'model__class_weight': ['balanced', None],\n",
        "        'model__solver': ['liblinear', 'saga']\n",
        "    }\n",
        "\n",
        "    lr_grid = GridSearchCV(lr_pipeline, lr_param_grid, cv=3, scoring='roc_auc')\n",
        "    lr_grid.fit(X_train, y_train)\n",
        "    print(f\"Best Logistic Regression parameters: {lr_grid.best_params_}\")\n",
        "    print(f\"Best ROC AUC score: {lr_grid.best_score_:.3f}\")\n",
        "\n",
        "    # Random Forest\n",
        "    print(\"\\nTuning Random Forest...\")\n",
        "    rf_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('model', RandomForestClassifier())\n",
        "    ])\n",
        "\n",
        "    rf_param_grid = {\n",
        "        'model__n_estimators': [50, 100, 200],\n",
        "        'model__max_depth': [None, 10, 20],\n",
        "        'model__min_samples_split': [2, 5, 10],\n",
        "        'model__class_weight': ['balanced', 'balanced_subsample', None]\n",
        "    }\n",
        "\n",
        "    rf_grid = GridSearchCV(rf_pipeline, rf_param_grid, cv=3, scoring='roc_auc')\n",
        "    rf_grid.fit(X_train, y_train)\n",
        "    print(f\"Best Random Forest parameters: {rf_grid.best_params_}\")\n",
        "    print(f\"Best ROC AUC score: {rf_grid.best_score_:.3f}\")\n",
        "\n",
        "    # XGBoost\n",
        "    print(\"\\nTuning XGBoost...\")\n",
        "    xgb_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('model', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
        "    ])\n",
        "\n",
        "    xgb_param_grid = {\n",
        "        'model__n_estimators': [50, 100, 200],\n",
        "        'model__learning_rate': [0.01, 0.1, 0.3],\n",
        "        'model__max_depth': [3, 5, 7],\n",
        "        'model__scale_pos_weight': [1, 5, 25]\n",
        "    }\n",
        "\n",
        "    xgb_grid = GridSearchCV(xgb_pipeline, xgb_param_grid, cv=3, scoring='roc_auc')\n",
        "    xgb_grid.fit(X_train, y_train)\n",
        "    print(f\"Best XGBoost parameters: {xgb_grid.best_params_}\")\n",
        "    print(f\"Best ROC AUC score: {xgb_grid.best_score_:.3f}\")\n",
        "\n",
        "    # LightGBM with verbosity fixed\n",
        "    print(\"\\nTuning LightGBM...\")\n",
        "    lgb_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('model', lgb.LGBMClassifier(verbosity=-1))  # Suppress LightGBM warnings\n",
        "    ])\n",
        "\n",
        "    lgb_param_grid = {\n",
        "        'model__n_estimators': [50, 100, 200],\n",
        "        'model__learning_rate': [0.01, 0.1, 0.3],\n",
        "        'model__num_leaves': [31, 50, 100],\n",
        "        'model__class_weight': ['balanced', None]\n",
        "    }\n",
        "\n",
        "    lgb_grid = GridSearchCV(lgb_pipeline, lgb_param_grid, cv=3, scoring='roc_auc')\n",
        "    lgb_grid.fit(X_train, y_train)\n",
        "    print(f\"Best LightGBM parameters: {lgb_grid.best_params_}\")\n",
        "    print(f\"Best ROC AUC score: {lgb_grid.best_score_:.3f}\")\n",
        "\n",
        "    best_models = {\n",
        "        'Logistic Regression': lr_grid.best_estimator_,\n",
        "        'Random Forest': rf_grid.best_estimator_,\n",
        "        'XGBoost': xgb_grid.best_estimator_,\n",
        "        'LightGBM': lgb_grid.best_estimator_\n",
        "    }\n",
        "    return best_models"
      ],
      "metadata": {
        "id": "uHp8BXEACtxo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "\n",
        "def train_models(X, y, tuned_models=None):\n",
        "    if tuned_models:\n",
        "        print(\"\\nUsing tuned models...\")\n",
        "        return tuned_models\n",
        "\n",
        "    print(\"\\nTraining models with default parameters...\")\n",
        "    models = {\n",
        "        'Logistic Regression': Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('model', LogisticRegression(class_weight='balanced', max_iter=1000))\n",
        "        ]),\n",
        "        'Random Forest': Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('model', RandomForestClassifier(class_weight='balanced'))\n",
        "        ]),\n",
        "        'XGBoost': Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('model', XGBClassifier(scale_pos_weight=25, use_label_encoder=False, eval_metric='logloss'))\n",
        "        ]),\n",
        "        'LightGBM': Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            # Set verbosity=-1 to suppress warning messages\n",
        "            ('model', lgb.LGBMClassifier(class_weight='balanced', verbosity=-1))\n",
        "        ])\n",
        "    }\n",
        "\n",
        "    trained = {}\n",
        "    for name, model in models.items():\n",
        "        model.fit(X, y)\n",
        "        trained[name] = model\n",
        "    return trained"
      ],
      "metadata": {
        "id": "grDo5cq5H7oK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance\n",
        "\n",
        "def get_top_features(models, feature_names, top_n=5):\n",
        "    xgb_imp = models['XGBoost'].named_steps['model'].feature_importances_\n",
        "    lgbm_imp = models['LightGBM'].named_steps['model'].feature_importances_\n",
        "    df_imp = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'XGBoost': xgb_imp,\n",
        "        'LightGBM': lgbm_imp\n",
        "    })\n",
        "    df_imp['Average'] = df_imp[['XGBoost', 'LightGBM']].mean(axis=1)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    top_features_df = df_imp.sort_values('Average', ascending=False).head(15)\n",
        "    plt.barh(top_features_df['Feature'], top_features_df['Average'])\n",
        "    plt.xlabel('Average Importance')\n",
        "    plt.ylabel('Feature')\n",
        "    plt.title('Top 15 Features by Importance')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"plots/feature_importance.png\")\n",
        "    plt.close()\n",
        "\n",
        "    return df_imp.sort_values('Average', ascending=False).head(top_n)['Feature'].tolist()"
      ],
      "metadata": {
        "id": "HBoU54EGIYHn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(f\"ROC AUC: {roc_auc_score(y_test, y_proba):.3f}\")\n",
        "    return {\n",
        "        'roc_auc': roc_auc_score(y_test, y_proba),\n",
        "        'classification_report': classification_report(y_test, y_pred, output_dict=True)\n",
        "    }\n",
        "\n",
        "def plot_roc_curves(models, X_test, y_test):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    for name, model in models.items():\n",
        "        y_proba = model.predict_proba(X_test)[:, 1]\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label=f\"{name} (AUC = {roc_auc:.2f})\")\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC Curve Comparison\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    os.makedirs(\"plots\", exist_ok=True)\n",
        "    plt.savefig(\"plots/roc_curves.png\")\n",
        "    plt.close()\n",
        "    return {name: auc(roc_curve(y_test, model.predict_proba(X_test)[:, 1])[0],\n",
        "                     roc_curve(y_test, model.predict_proba(X_test)[:, 1])[1])\n",
        "            for name, model in models.items()}"
      ],
      "metadata": {
        "id": "XIX82y1zIYKe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Business Recommendations\n",
        "def generate_business_recommendations(models, feature_names, evaluation_metrics, cluster_interpretations=None):\n",
        "    print(\"\\n===== BUSINESS RECOMMENDATIONS =====\")\n",
        "    best_model_name = max(evaluation_metrics, key=lambda x: evaluation_metrics[x])\n",
        "    best_auc = evaluation_metrics[best_model_name]\n",
        "    print(f\"The best performing model is {best_model_name} with ROC AUC of {best_auc:.3f}\")\n",
        "\n",
        "    if best_model_name in ['XGBoost', 'LightGBM']:\n",
        "        model_importances = models[best_model_name].named_steps['model'].feature_importances_\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'Feature': feature_names,\n",
        "            'Importance': model_importances\n",
        "        }).sort_values('Importance', ascending=False)\n",
        "        top_10_features = feature_importance.head(10)\n",
        "        print(\"\\nTop 10 Factors Influencing Churn:\")\n",
        "        for i, (feature, importance) in enumerate(zip(top_10_features['Feature'], top_10_features['Importance']), 1):\n",
        "            print(f\"{i}. {feature}: {importance:.4f}\")\n",
        "\n",
        "    recommendations = []\n",
        "    recommendations.append({\n",
        "        'category': 'Strategic',\n",
        "        'title': 'Implement Predictive Churn Prevention Program',\n",
        "        'description': f'Deploy the {best_model_name} model as part of a proactive customer retention system. This model achieves {best_auc:.1%} AUC and can identify at-risk users before they churn.',\n",
        "        'implementation': 'Create an automated alert system that flags users with high churn probability for immediate retention interventions.'\n",
        "    })\n",
        "\n",
        "    if 'membership_days' in feature_names:\n",
        "        recommendations.append({\n",
        "            'category': 'New Users',\n",
        "            'title': 'Enhanced Onboarding Experience',\n",
        "            'description': 'Users with low membership days are at higher risk of churning. Focus on improving the first 30 days of user experience.',\n",
        "            'implementation': 'Create a structured onboarding program with guided tutorials, personalized recommendations, and early engagement incentives.'\n",
        "        })\n",
        "\n",
        "    if 'total_listen' in feature_names or 'avg_listen' in feature_names:\n",
        "        recommendations.append({\n",
        "            'category': 'Engagement',\n",
        "            'title': 'Listening Activity Incentives',\n",
        "            'description': 'Users with lower listening activity are more likely to churn. Increase engagement through personalized content.',\n",
        "            'implementation': 'Develop a \"Weekly Discovery\" feature that introduces users to new content aligned with their preferences.'\n",
        "        })\n",
        "\n",
        "    if 'unique_artists' in feature_names or 'unique_songs' in feature_names:\n",
        "        recommendations.append({\n",
        "            'category': 'Content Diversity',\n",
        "            'title': 'Content Diversity Program',\n",
        "            'description': 'Users who explore more diverse content tend to remain engaged longer. Encourage exploration of new artists and songs.',\n",
        "            'implementation': 'Create curated playlists that gradually introduce users to new genres and artists similar to their current preferences.'\n",
        "        })\n",
        "\n",
        "    if 'is_paid' in feature_names:\n",
        "        recommendations.append({\n",
        "            'category': 'Subscription',\n",
        "            'title': 'Subscription Tier Optimization',\n",
        "            'description': 'Different patterns of churn exist between free and paid users. Tailor retention strategies to each segment.',\n",
        "            'implementation': 'For free users, highlight premium features through targeted campaigns. For paid users, introduce loyalty rewards and exclusive content.'\n",
        "        })\n",
        "\n",
        "    if 'is_error_sum' in feature_names:\n",
        "        recommendations.append({\n",
        "            'category': 'Technical',\n",
        "            'title': 'Error Reduction Initiative',\n",
        "            'description': 'Technical errors correlate with increased churn rates. Prioritize fixing common errors and improve error handling.',\n",
        "            'implementation': 'Develop a system to track and prioritize errors by their impact on user experience. Create more user-friendly error messages.'\n",
        "        })\n",
        "\n",
        "    if cluster_interpretations:\n",
        "        high_risk_clusters = [c for c, interp in cluster_interpretations.items()\n",
        "                             if 'High Risk' in interp['description']]\n",
        "        if high_risk_clusters:\n",
        "            high_risk = high_risk_clusters[0]\n",
        "            interp = cluster_interpretations[high_risk]\n",
        "            recommendations.append({\n",
        "                'category': 'Targeted Intervention',\n",
        "                'title': f'High-Risk Segment Focus: {interp[\"description\"]}',\n",
        "                'description': f'Cluster {high_risk} has a churn rate of {interp[\"churn_rate\"]} and represents a critical segment for intervention.',\n",
        "                'implementation': '; '.join(interp['recommendations'])\n",
        "            })\n",
        "\n",
        "    recommendations.append({\n",
        "        'category': 'Retention',\n",
        "        'title': 'Proactive Retention Program',\n",
        "        'description': 'Implement a structured retention program targeting users before they show explicit signs of churning.',\n",
        "        'implementation': 'Create automated journeys with targeted communications at key risk points (e.g., after 30 days of inactivity).'\n",
        "    })\n",
        "\n",
        "    print(\"\\n----- Actionable Business Recommendations -----\")\n",
        "    with open(\"business_recommendations.txt\", \"w\") as f:\n",
        "        f.write(\"===== ACTIONABLE BUSINESS RECOMMENDATIONS =====\\n\\n\")\n",
        "        for i, rec in enumerate(recommendations, 1):\n",
        "            print(f\"\\n{i}. [{rec['category']}] {rec['title']}\")\n",
        "            print(f\"   {rec['description']}\")\n",
        "            print(f\"   Implementation: {rec['implementation']}\")\n",
        "            f.write(f\"{i}. [{rec['category']}] {rec['title']}\\n\")\n",
        "            f.write(f\"   {rec['description']}\\n\")\n",
        "            f.write(f\"   Implementation: {rec['implementation']}\\n\\n\")\n",
        "    return recommendations"
      ],
      "metadata": {
        "id": "LnOyiq5aIYNS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    print(\"======================================================\")\n",
        "    print(\"   CUSTOMER CHURN PREDICTION AND ANALYSIS PROJECT     \")\n",
        "    print(\"======================================================\")\n",
        "    print(f\"Run date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    print(\"\\nLoading data...\")\n",
        "    df = load_data('/content/drive/MyDrive/data.json')\n",
        "\n",
        "    # New: Perform EDA\n",
        "    df = perform_eda(df)\n",
        "\n",
        "    # Preprocessing & Feature Engineering\n",
        "    user_features = preprocess_data(df)\n",
        "\n",
        "    # Clustering with detailed interpretation\n",
        "    user_features = cluster_users(user_features)\n",
        "\n",
        "    # Visualization\n",
        "    visualize_data(user_features)\n",
        "\n",
        "    # Prepare data for modeling\n",
        "    all_features = [col for col in user_features.columns if col not in ['userId', 'churn']]\n",
        "    target = 'churn'\n",
        "\n",
        "    X = user_features[all_features]\n",
        "    y = user_features[target]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
        "\n",
        "    # New: Hyperparameter tuning\n",
        "    tuned_models = tune_hyperparameters(X_train, y_train)\n",
        "\n",
        "    # Train models using tuned parameters\n",
        "    trained_models = train_models(X_train, y_train, tuned_models=tuned_models)\n",
        "\n",
        "    # Extract top features\n",
        "    print(\"\\nExtracting top features from models...\")\n",
        "    top_features = get_top_features(trained_models, all_features, top_n=10)\n",
        "    print(f\"Top 10 Features: {top_features}\")\n",
        "\n",
        "    # Retrain models using only top features\n",
        "    print(\"\\nRetraining models using only top 10 features...\")\n",
        "    trained_top = train_models(X_train[top_features], y_train, tuned_models=None)\n",
        "\n",
        "    # Evaluate models\n",
        "    print(\"\\nFinal Evaluation on test set using top 10 features:\")\n",
        "    evaluation_results = {}\n",
        "    for name, model in trained_top.items():\n",
        "        print(f\"\\n{name}\")\n",
        "        evaluation_results[name] = evaluate_model(model, X_test[top_features], y_test)['roc_auc']\n",
        "\n",
        "    # Plot ROC curves\n",
        "    print(\"\\nPlotting ROC Curves for all models:\")\n",
        "    auc_scores = plot_roc_curves(trained_top, X_test[top_features], y_test)\n",
        "\n",
        "    # Generate business recommendations\n",
        "    cluster_interpretations = None\n",
        "    if 'cluster' in user_features.columns:\n",
        "        # Extract cluster interpretations if available\n",
        "        cluster_analysis = user_features.groupby('cluster').agg({\n",
        "            'churn': 'mean',\n",
        "            'total_listen': 'mean',\n",
        "            'avg_listen': 'mean',\n",
        "            'sessions': 'mean',\n",
        "            'active_duration': 'mean',\n",
        "            'membership_days': 'mean',\n",
        "            'is_paid': 'mean',\n",
        "            'unique_artists': 'mean',\n",
        "            'unique_songs': 'mean',\n",
        "            'avg_session_duration': 'mean',\n",
        "            'userId': 'count'\n",
        "        }).rename(columns={'userId': 'count'})\n",
        "\n",
        "        cluster_interpretations, _ = interpret_clusters(cluster_analysis)\n",
        "\n",
        "    generate_business_recommendations(trained_top, top_features, auc_scores, cluster_interpretations)\n",
        "\n",
        "    print(\"\\nAnalysis complete. Results saved to output files and plots.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FPsgA_RLZIQ",
        "outputId": "3f678810-f5d8-4468-8c9b-f76535ab9705"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================\n",
            "   CUSTOMER CHURN PREDICTION AND ANALYSIS PROJECT     \n",
            "======================================================\n",
            "Run date: 2025-04-14 23:06:05\n",
            "\n",
            "Loading data...\n",
            "\n",
            "===== EXPLORATORY DATA ANALYSIS =====\n",
            "Dataset shape: (26259199, 18)\n",
            "\n",
            "Data types:\n",
            "status             int64\n",
            "gender            object\n",
            "length           float64\n",
            "firstName         object\n",
            "level             object\n",
            "lastName          object\n",
            "registration     float64\n",
            "userId            object\n",
            "ts                 int64\n",
            "auth              object\n",
            "page              object\n",
            "sessionId          int64\n",
            "location          object\n",
            "itemInSession      int64\n",
            "userAgent         object\n",
            "song              object\n",
            "artist            object\n",
            "method            object\n",
            "dtype: object\n",
            "\n",
            "Missing values per column:\n",
            "status                 0\n",
            "gender            778479\n",
            "length           5408927\n",
            "firstName         778479\n",
            "level                  0\n",
            "lastName          778479\n",
            "registration      778479\n",
            "userId                 0\n",
            "ts                     0\n",
            "auth                   0\n",
            "page                   0\n",
            "sessionId              0\n",
            "location          778479\n",
            "itemInSession          0\n",
            "userAgent         778479\n",
            "song             5408927\n",
            "artist           5408927\n",
            "method                 0\n",
            "dtype: int64\n",
            "\n",
            "Basic statistics:\n",
            "             status        length  registration            ts     sessionId  \\\n",
            "count  2.625920e+07  2.085027e+07  2.548072e+07  2.625920e+07  2.625920e+07   \n",
            "mean   2.100677e+02  2.487254e+02  1.535221e+12  1.540906e+12  1.005780e+05   \n",
            "std    3.155073e+01  9.728710e+01  3.240299e+09  1.515811e+09  7.190921e+04   \n",
            "min    2.000000e+02  5.220000e-01  1.508019e+12  1.538352e+12  1.000000e+00   \n",
            "25%    2.000000e+02  1.998885e+02  1.534020e+12  1.539624e+12  3.022800e+04   \n",
            "50%    2.000000e+02  2.340828e+02  1.536196e+12  1.540872e+12  9.488500e+04   \n",
            "75%    2.000000e+02  2.768714e+02  1.537477e+12  1.542192e+12  1.639290e+05   \n",
            "max    4.040000e+02  3.024666e+03  1.543822e+12  1.543622e+12  2.403810e+05   \n",
            "\n",
            "       itemInSession  \n",
            "count   2.625920e+07  \n",
            "mean    1.065627e+02  \n",
            "std     1.176581e+02  \n",
            "min     0.000000e+00  \n",
            "25%     2.600000e+01  \n",
            "50%     6.700000e+01  \n",
            "75%     1.460000e+02  \n",
            "max     1.428000e+03  \n",
            "\n",
            "Unique values in categorical columns:\n",
            "gender: 2 unique values\n",
            "firstName: 5467 unique values\n",
            "level: 2 unique values\n",
            "lastName: 1000 unique values\n",
            "userId: 22278 unique values\n",
            "auth: 4 unique values\n",
            "page: 22 unique values\n",
            "location: 886 unique values\n",
            "userAgent: 85 unique values\n",
            "song: 253564 unique values\n",
            "artist: 38337 unique values\n",
            "method: 2 unique values\n",
            "\n",
            "Page distribution:\n",
            "page\n",
            "NextSong                     20850272\n",
            "Home                          1343102\n",
            "Thumbs Up                     1151465\n",
            "Add to Playlist                597921\n",
            "Roll Advert                    385212\n",
            "Add Friend                     381664\n",
            "Login                          296350\n",
            "Logout                         296005\n",
            "Thumbs Down                    239212\n",
            "Downgrade                      184240\n",
            "Help                           155100\n",
            "Settings                       147074\n",
            "About                           92759\n",
            "Upgrade                         50507\n",
            "Save Settings                   29516\n",
            "Error                           25962\n",
            "Submit Upgrade                  15135\n",
            "Submit Downgrade                 6494\n",
            "Cancel                           5003\n",
            "Cancellation Confirmation        5003\n",
            "Register                          802\n",
            "Submit Registration               401\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Preprocessing & Feature Engineering...\n",
            "Created 41 features for 22278 users\n",
            "\n",
            "Clustering users into behavioral segments...\n",
            "Optimal number of clusters: 10\n",
            "\n",
            "===== CLUSTER ANALYSIS =====\n",
            "            churn  total_listen  avg_listen       sessions  active_duration  \\\n",
            "cluster                                                                       \n",
            "7        0.498623  3.341856e+05  248.896532      14.853444     3.345055e+06   \n",
            "9        0.374513  1.462558e+05  248.841407       7.901224     2.955527e+06   \n",
            "6        0.354962  1.091555e+04  244.049948       1.000000     1.085446e+04   \n",
            "8        0.243454  4.696261e+04  248.711640       5.179387     2.839465e+06   \n",
            "0        0.195786  1.683069e+04  248.233689       3.273924     2.296480e+06   \n",
            "2        0.185731  6.195457e+05  248.770140      29.913910     4.777010e+06   \n",
            "5        0.151399  1.193728e+06  248.857208      55.830789     5.029135e+06   \n",
            "3        0.092881  1.012239e+05  248.751509      10.326780     4.355150e+06   \n",
            "1        0.054222  2.816580e+05  248.708981      17.589965     4.835845e+06   \n",
            "4        0.000000  0.000000e+00    0.000000  158115.000000     5.270381e+06   \n",
            "\n",
            "         membership_days  is_paid  unique_artists  unique_songs  \\\n",
            "cluster                                                           \n",
            "7                    0.0      0.0      964.525069   1214.505234   \n",
            "9                    0.0      0.0      484.549527    556.556761   \n",
            "6                    0.0      0.0       42.405534     43.535305   \n",
            "8                    0.0      0.0      174.825070    185.089136   \n",
            "0                    0.0      0.0       65.637401     67.210272   \n",
            "2                    0.0      0.0     1563.023914   2145.068553   \n",
            "5                    0.0      0.0     2496.996183   3830.171756   \n",
            "3                    0.0      0.0      352.522712    391.463051   \n",
            "1                    0.0      0.0      841.961694   1036.899110   \n",
            "4                    0.0      0.0        0.000000      0.000000   \n",
            "\n",
            "         avg_session_duration  count  \n",
            "cluster                               \n",
            "7                61191.274413   1815  \n",
            "9                41329.797994   3594  \n",
            "6                10854.464695   1048  \n",
            "8                17742.886020   3590  \n",
            "0                 8021.200217   2278  \n",
            "2                49008.409119   2509  \n",
            "5                48854.901421    786  \n",
            "3                19453.381013   2950  \n",
            "1                34787.847820   3707  \n",
            "4               594377.677981      1  \n",
            "\n",
            "===== CLUSTER INTERPRETATION =====\n",
            "\n",
            "Cluster 7: High Risk - Free users with Low activity, New, Diverse taste\n",
            "Size: 1815.0 users\n",
            "Churn Rate: 49.86%\n",
            "Recommendations:\n",
            "  1. Engage with targeted content recommendations\n",
            "  2. Send re-engagement emails with personalized playlists\n",
            "  3. Improve onboarding experience\n",
            "\n",
            "Cluster 9: Medium Risk - Free users with Low activity, New, Diverse taste\n",
            "Size: 3594.0 users\n",
            "Churn Rate: 37.45%\n",
            "Recommendations:\n",
            "  1. Create personalized discovery playlists\n",
            "  2. Highlight premium benefits\n",
            "\n",
            "Cluster 6: Medium Risk - Free users with Low activity, New, Focused taste\n",
            "Size: 1048.0 users\n",
            "Churn Rate: 35.50%\n",
            "Recommendations:\n",
            "  1. Recommend new artists similar to their favorites\n",
            "  2. Highlight premium benefits\n",
            "\n",
            "Cluster 8: Medium Risk - Free users with Low activity, New, Focused taste\n",
            "Size: 3590.0 users\n",
            "Churn Rate: 24.35%\n",
            "Recommendations:\n",
            "  1. Recommend new artists similar to their favorites\n",
            "  2. Highlight premium benefits\n",
            "\n",
            "Cluster 0: Low Risk - Free users with Low activity, New, Focused taste\n",
            "Size: 2278.0 users\n",
            "Churn Rate: 19.58%\n",
            "Recommendations:\n",
            "  1. Maintain engagement with new features\n",
            "  2. Leverage these users for referrals\n",
            "\n",
            "Cluster 2: Low Risk - Free users with High activity, New, Diverse taste\n",
            "Size: 2509.0 users\n",
            "Churn Rate: 18.57%\n",
            "Recommendations:\n",
            "  1. Maintain engagement with new features\n",
            "  2. Leverage these users for referrals\n",
            "\n",
            "Cluster 5: Low Risk - Free users with High activity, New, Diverse taste\n",
            "Size: 786.0 users\n",
            "Churn Rate: 15.14%\n",
            "Recommendations:\n",
            "  1. Maintain engagement with new features\n",
            "  2. Leverage these users for referrals\n",
            "\n",
            "Cluster 3: Low Risk - Free users with High activity, New, Focused taste\n",
            "Size: 2950.0 users\n",
            "Churn Rate: 9.29%\n",
            "Recommendations:\n",
            "  1. Maintain engagement with new features\n",
            "  2. Leverage these users for referrals\n",
            "\n",
            "Cluster 1: Low Risk - Free users with High activity, New, Diverse taste\n",
            "Size: 3707.0 users\n",
            "Churn Rate: 5.42%\n",
            "Recommendations:\n",
            "  1. Maintain engagement with new features\n",
            "  2. Leverage these users for referrals\n",
            "\n",
            "Cluster 4: Low Risk - Free users with High activity, New, Focused taste\n",
            "Size: 1.0 users\n",
            "Churn Rate: 0.00%\n",
            "Recommendations:\n",
            "  1. Maintain engagement with new features\n",
            "  2. Leverage these users for referrals\n",
            "\n",
            "Generating visualizations...\n",
            "\n",
            "===== HYPERPARAMETER TUNING =====\n",
            "\n",
            "Tuning Logistic Regression...\n",
            "Best Logistic Regression parameters: {'model__C': 1, 'model__class_weight': 'balanced', 'model__solver': 'liblinear'}\n",
            "Best ROC AUC score: 0.906\n",
            "\n",
            "Tuning Random Forest...\n",
            "Best Random Forest parameters: {'model__class_weight': None, 'model__max_depth': None, 'model__min_samples_split': 10, 'model__n_estimators': 200}\n",
            "Best ROC AUC score: 0.976\n",
            "\n",
            "Tuning XGBoost...\n",
            "Best XGBoost parameters: {'model__learning_rate': 0.3, 'model__max_depth': 3, 'model__n_estimators': 200, 'model__scale_pos_weight': 5}\n",
            "Best ROC AUC score: 0.994\n",
            "\n",
            "Tuning LightGBM...\n",
            "Best LightGBM parameters: {'model__class_weight': 'balanced', 'model__learning_rate': 0.1, 'model__n_estimators': 200, 'model__num_leaves': 31}\n",
            "Best ROC AUC score: 0.993\n",
            "\n",
            "Using tuned models...\n",
            "\n",
            "Extracting top features from models...\n",
            "Top 10 Features: ['unique_pages', 'recency', 'is_settings_sum', 'is_upgrade_sum', 'is_downgrade_sum', 'avg_listen', 'avg_session_items', 'active_duration', 'is_about_sum', 'std_listen']\n",
            "\n",
            "Retraining models using only top 10 features...\n",
            "\n",
            "Training models with default parameters...\n",
            "\n",
            "Final Evaluation on test set using top 10 features:\n",
            "\n",
            "Logistic Regression\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.93      0.94      5183\n",
            "           1       0.78      0.87      0.82      1501\n",
            "\n",
            "    accuracy                           0.92      6684\n",
            "   macro avg       0.87      0.90      0.88      6684\n",
            "weighted avg       0.92      0.92      0.92      6684\n",
            "\n",
            "ROC AUC: 0.958\n",
            "\n",
            "Random Forest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.98      0.97      5183\n",
            "           1       0.94      0.85      0.89      1501\n",
            "\n",
            "    accuracy                           0.95      6684\n",
            "   macro avg       0.95      0.92      0.93      6684\n",
            "weighted avg       0.95      0.95      0.95      6684\n",
            "\n",
            "ROC AUC: 0.981\n",
            "\n",
            "XGBoost\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97      5183\n",
            "           1       0.86      0.92      0.89      1501\n",
            "\n",
            "    accuracy                           0.95      6684\n",
            "   macro avg       0.92      0.94      0.93      6684\n",
            "weighted avg       0.95      0.95      0.95      6684\n",
            "\n",
            "ROC AUC: 0.987\n",
            "\n",
            "LightGBM\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.97      0.97      5183\n",
            "           1       0.89      0.93      0.91      1501\n",
            "\n",
            "    accuracy                           0.96      6684\n",
            "   macro avg       0.94      0.95      0.94      6684\n",
            "weighted avg       0.96      0.96      0.96      6684\n",
            "\n",
            "ROC AUC: 0.989\n",
            "\n",
            "Plotting ROC Curves for all models:\n",
            "\n",
            "===== CLUSTER INTERPRETATION =====\n",
            "\n",
            "Cluster 0: Low Risk - Free users with Low activity, New, Focused taste\n",
            "Size: 2278.0 users\n",
            "Churn Rate: 19.58%\n",
            "Recommendations:\n",
            "  1. Maintain engagement with new features\n",
            "  2. Leverage these users for referrals\n",
            "\n",
            "Cluster 1: Low Risk - Free users with High activity, New, Diverse taste\n",
            "Size: 3707.0 users\n",
            "Churn Rate: 5.42%\n",
            "Recommendations:\n",
            "  1. Maintain engagement with new features\n",
            "  2. Leverage these users for referrals\n",
            "\n",
            "Cluster 2: Low Risk - Free users with High activity, New, Diverse taste\n",
            "Size: 2509.0 users\n",
            "Churn Rate: 18.57%\n",
            "Recommendations:\n",
            "  1. Maintain engagement with new features\n",
            "  2. Leverage these users for referrals\n",
            "\n",
            "Cluster 3: Low Risk - Free users with High activity, New, Focused taste\n",
            "Size: 2950.0 users\n",
            "Churn Rate: 9.29%\n",
            "Recommendations:\n",
            "  1. Maintain engagement with new features\n",
            "  2. Leverage these users for referrals\n",
            "\n",
            "Cluster 4: Low Risk - Free users with High activity, New, Focused taste\n",
            "Size: 1.0 users\n",
            "Churn Rate: 0.00%\n",
            "Recommendations:\n",
            "  1. Maintain engagement with new features\n",
            "  2. Leverage these users for referrals\n",
            "\n",
            "Cluster 5: Low Risk - Free users with High activity, New, Diverse taste\n",
            "Size: 786.0 users\n",
            "Churn Rate: 15.14%\n",
            "Recommendations:\n",
            "  1. Maintain engagement with new features\n",
            "  2. Leverage these users for referrals\n",
            "\n",
            "Cluster 6: Medium Risk - Free users with Low activity, New, Focused taste\n",
            "Size: 1048.0 users\n",
            "Churn Rate: 35.50%\n",
            "Recommendations:\n",
            "  1. Recommend new artists similar to their favorites\n",
            "  2. Highlight premium benefits\n",
            "\n",
            "Cluster 7: High Risk - Free users with Low activity, New, Diverse taste\n",
            "Size: 1815.0 users\n",
            "Churn Rate: 49.86%\n",
            "Recommendations:\n",
            "  1. Engage with targeted content recommendations\n",
            "  2. Send re-engagement emails with personalized playlists\n",
            "  3. Improve onboarding experience\n",
            "\n",
            "Cluster 8: Medium Risk - Free users with Low activity, New, Focused taste\n",
            "Size: 3590.0 users\n",
            "Churn Rate: 24.35%\n",
            "Recommendations:\n",
            "  1. Recommend new artists similar to their favorites\n",
            "  2. Highlight premium benefits\n",
            "\n",
            "Cluster 9: Medium Risk - Free users with Low activity, New, Diverse taste\n",
            "Size: 3594.0 users\n",
            "Churn Rate: 37.45%\n",
            "Recommendations:\n",
            "  1. Create personalized discovery playlists\n",
            "  2. Highlight premium benefits\n",
            "\n",
            "===== BUSINESS RECOMMENDATIONS =====\n",
            "The best performing model is LightGBM with ROC AUC of 0.989\n",
            "\n",
            "Top 10 Factors Influencing Churn:\n",
            "1. unique_pages: 704.0000\n",
            "2. recency: 457.0000\n",
            "3. avg_session_items: 408.0000\n",
            "4. active_duration: 281.0000\n",
            "5. is_upgrade_sum: 227.0000\n",
            "6. avg_listen: 203.0000\n",
            "7. std_listen: 197.0000\n",
            "8. is_downgrade_sum: 197.0000\n",
            "9. is_settings_sum: 191.0000\n",
            "10. is_about_sum: 135.0000\n",
            "\n",
            "----- Actionable Business Recommendations -----\n",
            "\n",
            "1. [Strategic] Implement Predictive Churn Prevention Program\n",
            "   Deploy the LightGBM model as part of a proactive customer retention system. This model achieves 98.9% AUC and can identify at-risk users before they churn.\n",
            "   Implementation: Create an automated alert system that flags users with high churn probability for immediate retention interventions.\n",
            "\n",
            "2. [Engagement] Listening Activity Incentives\n",
            "   Users with lower listening activity are more likely to churn. Increase engagement through personalized content.\n",
            "   Implementation: Develop a \"Weekly Discovery\" feature that introduces users to new content aligned with their preferences.\n",
            "\n",
            "3. [Targeted Intervention] High-Risk Segment Focus: High Risk - Free users with Low activity, New, Diverse taste\n",
            "   Cluster 7 has a churn rate of 49.86% and represents a critical segment for intervention.\n",
            "   Implementation: Engage with targeted content recommendations; Send re-engagement emails with personalized playlists; Improve onboarding experience\n",
            "\n",
            "4. [Retention] Proactive Retention Program\n",
            "   Implement a structured retention program targeting users before they show explicit signs of churning.\n",
            "   Implementation: Create automated journeys with targeted communications at key risk points (e.g., after 30 days of inactivity).\n",
            "\n",
            "Analysis complete. Results saved to output files and plots.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ygFCP4YzLafg"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}